{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Podstawy - propagacja wsteczna, metoda gradientu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stwórzmy jakiś zbiór, którego nie da się łatwo \"uchwycić\" metodami liniowymi. Najprostszym pomysłem są okręgi o różnych promieniach, i klasie zależnej od promienia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "points_per_class = 200\n",
    "X = np.zeros((points_per_class*n_classes, 2))\n",
    "Y = np.zeros(points_per_class*n_classes, dtype=int)\n",
    "for i in range(n_classes):\n",
    "    idx = range(points_per_class*i,points_per_class*(i+1))\n",
    "    \n",
    "    r = i + np.random.random(points_per_class)* 0.5 # \n",
    "    t = np.random.random(points_per_class) *2* np.pi\n",
    "    \n",
    "    X[idx] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "    Y[idx] = int(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=Y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pierwszą próbę klasyfikacji zrobimy za pomocą prostego klasyfikatora liniowego, to znaczy bezpośrednio po warstwie wejściowej mamy 3-elementową warstwę wyjściową reprezentującą prawdopobieństwo przynależności do danej klasy.\n",
    "\n",
    "Postać macierzowa:\n",
    "$$ y = f(U),$$\n",
    "$$ U = XW +b $$\n",
    "\n",
    "gdzie $f$ jest funkcją softmax:,\n",
    "\n",
    "$$ \\hat{y_j} = \\sigma (u)_j = \\frac{\\exp{u_j}}{\\sum_{j = 1}^{k} \\exp{u_j}} $$\n",
    "\n",
    "dla $j = 1, ..., k$.\n",
    "\n",
    "Będziemy optymalizować funkcję straty \n",
    "$$ L(y, \\hat{y}) = -\\frac{1}{N} \\sum_{n} y_n\\log{\\hat{y_n}}  $$\n",
    "\n",
    "Przydatna rzecz: \n",
    "$$ \\frac{\\partial L}{\\partial u_j} = \\hat{y_j} - y_j $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# randomizacja wartości początkowych\n",
    "D = 2\n",
    "K = 3\n",
    "W = 1 * np.random.randn(D,K)\n",
    "b = np.zeros((1,K))\n",
    "\n",
    "# parametry \n",
    "step_size = 1e-0\n",
    "reg = 1e-2\n",
    "\n",
    "num_examples = X.shape[0]\n",
    "\n",
    "for i in range(200):  \n",
    "    # obliczamy scory oraz przynależności do klas przy aktualnych wagach\n",
    "    scores = np.dot(X, W) + b \n",
    "    \n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "    # obliczamy funkcję straty dla aktualnej predykcji\n",
    "    corect_logprobs = -np.log(probs[range(num_examples),Y])\n",
    "    data_loss = np.sum(corect_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W*W)\n",
    "    loss = data_loss + reg_loss\n",
    "    if i % 20 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, loss))\n",
    "\n",
    "    # wyliczamy gradient funkcji straty\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),Y] -= 1 # globalnie odejmujemy od obliczonych \n",
    "                                        # prawdopodobieństw wartość y_j, aby otrzymać gradient funkcji straty\n",
    "    dscores /= num_examples\n",
    "    \n",
    "    # it's backpropagation time!\n",
    "    dW = np.dot(X.T, dscores)\n",
    "    db = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "    # dodajemy jeszcze wpływ regularyzacji do gradientu wag\n",
    "    dW += reg * W\n",
    "\n",
    "    # aktualizujemy parametry\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predykcja - postępujemy tak, jak na początku kroku uczenia\n",
    "scores = np.dot(X, W) + b\n",
    "predicted_class = np.argmax(scores, axis=1) # nie musimy wyliczać konkretnych prawdopodobienstw - exp jest funkcją rosnącą\n",
    "print('overall accuracy: %.2f' % (np.mean(predicted_class == Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=predicted_class, s=40, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać, klasyfikator liniowy nie podołał zadaniu - co nie jest zaskoczeniem. Zobaczmy, jak sytuacja się zmieni po dodaniu jeszcze jednej warstwy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# POKAZAĆ co się dzieje przy dodaniu wag początkowych na 0\n",
    "# randomizacja wartości początkowych\n",
    "D = 2\n",
    "K = 3\n",
    "h = 12 # size of hidden layer\n",
    "W = 1 * np.random.randn(D,h) #np.zeros((D,K)) \n",
    "b = np.zeros((1,h))\n",
    "W2 = 1 * np.random.randn(h,K) #np.zeros((D,K)) \n",
    "b2 = np.zeros((1,K))\n",
    "\n",
    "# parametry \n",
    "step_size = 1e-0\n",
    "reg = 5e-2 # regularization strength\n",
    "\n",
    "\n",
    "num_examples = X.shape[0]\n",
    "for i in range(30):  \n",
    "    # obliczamy scory oraz przynależności do klas przy aktualnych wagach\n",
    "    hidden_layer = np.maximum(0, np.dot(X, W) + b) # warstwa ukryta - stosujemy funkcję aktywacyjną ReLU\n",
    "    scores = np.dot(hidden_layer, W2) + b2 # warstwa końcowa\n",
    "\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "    # obliczamy funkcję straty dla aktualnej predykcji\n",
    "    corect_logprobs = -np.log(probs[range(num_examples),Y])\n",
    "    data_loss = np.sum(corect_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W*W)\n",
    "    loss = data_loss + reg_loss\n",
    "    if i % 10 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, loss))\n",
    "\n",
    "    # wyliczamy gradient funkcji straty\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),Y] -= 1\n",
    "    dscores /= num_examples\n",
    "\n",
    "    # wsteczna propagacja gradientu\n",
    "    # najpierw badamy wpływ wag i stałej w ostatniej warstwie\n",
    "    dW2 = np.dot(hidden_layer.T, dscores)\n",
    "    db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "    \n",
    "    # następnie liczymy gradient dla wartości w warstwie ukrytej\n",
    "    dhidden = np.dot(dscores, W2.T)\n",
    "    # pamietamy o uwzględnieniu pochodnej funkcji aktywacyjnej - na szczęście jest dość prosta\n",
    "    dhidden[hidden_layer <= 0] = 0\n",
    "    # na koniec dostajemy gradienty dla pierwszej warstwy\n",
    "    dW = np.dot(X.T, dhidden)\n",
    "    db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "\n",
    "    # dodajemy jeszcze wpływ regularyzacji do gradientu wag\n",
    "    dW2 += reg * W2\n",
    "    dW += reg * W\n",
    "\n",
    "    # aktualizujemy parametry\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db\n",
    "    W2 += -step_size * dW2\n",
    "    b2 += -step_size * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_layer = np.maximum(0, np.dot(X, W) + b)\n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print('training accuracy: %.2f' % (np.mean(predicted_class == Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=predicted_class, s=40, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uf, dużo lepiej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv1D, Flatten\n",
    "from keras.optimizers import SGD, adam\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Kręgi w zbożu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najpierw zbudujemy sieci analogiczne do poprzednich - dzięki temu zobaczymy jak działają podstawowe funkcje i klasy, oraz przekonamy się jak bardzo user-friendly jest keras ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na pierwszy ogień - model liniowy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_linear = Sequential()\n",
    "model_linear.add(Dense(3, input_dim = 2)) # dla pierwszej warstwy musimy podać ilosć neuronów na wejściu - \n",
    "                                   # dla pozostałych nie musimy się martwić\n",
    "model_linear.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_linear.compile(optimizer=SGD(lr = 1),\n",
    "              loss='categorical_crossentropy') # parametry jak wcześniej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uczymy! \n",
    "model_linear.fit(X, to_categorical(Y), epochs=20, verbose = 0, batch_size=600) # batch_size=600 daje nam taki sam algorytm jak wcześniej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_linear = model_linear.predict(X)\n",
    "print('training accuracy: %.2f' % (np.mean(np.argmax(preds_linear, axis=1) == Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie model z jedną warstwą ukrytą."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_2l = Sequential()\n",
    "model_2l.add(Dense(12, input_dim=2))\n",
    "model_2l.add(Activation('relu'))\n",
    "model_2l.add(Dense(3))\n",
    "model_2l.add(Activation('softmax'))\n",
    "\n",
    "model_2l.compile(optimizer=SGD(lr = 1),\n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "# uczymy! \n",
    "model_2l.fit(X, to_categorical(Y), epochs=20, verbose = 0, batch_size=600) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_2l = model_2l.predict(X)\n",
    "print('training accuracy: %.2f' % (np.mean(np.argmax(preds_2l, axis=1) == Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bostońskie domostwa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing # pokazać opis danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train_boston, Y_train_boston), (X_test_boston, Y_test_boston) = boston_housing.load_data()\n",
    "print(X_train_boston.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_boston_linear = Sequential()\n",
    "model_boston_linear.add(Dense(1, input_dim=13))\n",
    "model_boston_linear.add(Activation('linear'))\n",
    "\n",
    "model_boston_linear.compile(optimizer=SGD(), loss=\"mean_squared_error\")\n",
    "\n",
    "model_boston_linear.fit(X_train_boston, Y_train_boston, epochs=50, batch_size=404, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_boston_linear.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wszystkie wagi wybuchają - trzeba coś z tym zrobić"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def describe_column(col):\n",
    "    return( (np.min(col), np.max(col), np.mean(col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats = np.apply_along_axis(describe_column, 0, X_train_boston)\n",
    "stats.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_boston /= stats[1]\n",
    "X_test_boston /= stats[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_boston_linear = Sequential()\n",
    "model_boston_linear.add(Dense(1, input_dim=13))\n",
    "model_boston_linear.add(Activation('linear'))\n",
    "\n",
    "model_boston_linear.compile(optimizer=\"sgd\", loss=\"mean_squared_error\")\n",
    "\n",
    "model_boston_linear.fit(X_train_boston, Y_train_boston, epochs=50, batch_size=404, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean((model_boston_linear.predict(X_test_boston).reshape(Y_test_boston.shape) - Y_test_boston)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_boston_2l = Sequential()\n",
    "model_boston_2l.add(Dense(12, input_dim=13))\n",
    "model_boston_2l.add(Activation('relu'))\n",
    "model_boston_2l.add(Dense(1))\n",
    "model_boston_2l.add(Activation('linear'))\n",
    "\n",
    "model_boston_2l.compile(optimizer=\"sgd\", loss=\"mean_squared_error\")\n",
    "\n",
    "model_boston_2l.fit(X_train_boston, Y_train_boston, epochs=50, batch_size=404, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.mean((model_boston_2l.predict(X_test_boston).reshape(Y_test_boston.shape) - Y_test_boston)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rozpoznawanie pisma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train_mnist, Y_train_mnist), (X_test_mnist, Y_test_mnist) = mnist.load_data()\n",
    "print(X_train_mnist.shape)\n",
    "print(Y_train_mnist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digit = X_train_mnist[0]\n",
    "plt.imshow(digit, interpolation = \"nearest\", cmap = \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizacja przez podzielenie przez maksimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do wypróbowania:\n",
    "    - subsampling obrazka \n",
    "        from scipy import ndimage\n",
    "        test_x2 = np.array([ndimage.interpolation.zoom(i, 18/28) for i in test_x])\n",
    "    - binaryzacja\n",
    "        X_train_mnist[X_train_mnist<cutoff] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.amax(X_train_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_mnist = X_train_mnist/255.0\n",
    "X_test_mnist = X_test_mnist/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_mnist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_mnist = X_train_mnist.reshape(60000, 28*28)\n",
    "X_test_mnist= X_test_mnist.reshape(X_test_mnist.shape[0], 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_mnist = Sequential()\n",
    "model_mnist.add(Dense(200, input_dim=28*28))\n",
    "model_mnist.add(Activation('relu'))\n",
    "model_mnist.add(Dense(10))\n",
    "model_mnist.add(Activation('softmax'))\n",
    "\n",
    "model_mnist.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "model_mnist.fit(X_train_mnist, to_categorical(Y_train_mnist), epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_mnist = model_mnist.predict(X_test_mnist)\n",
    "print('test accuracy: %.2f' % (np.mean(np.argmax(preds_mnist, axis=1) == Y_test_mnist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
